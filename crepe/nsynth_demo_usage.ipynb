{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from GANsynth_pytorch import NSynth\n",
    "\n",
    "import crepe\n",
    "from crepe.torch_backend import CREPE, build_and_load_model\n",
    "\n",
    "def expand_path(p: Union[str, Path]) -> Path:\n",
    "    return Path(p).expanduser().absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSynth_valid_json_data_path = expand_path(\n",
    "    '../../data/nsynth/valid/json_wav/examples.json')\n",
    "NSynth_valid_audio_path = expand_path(\n",
    "    '../../data/nsynth/valid/json_wav/audio/')\n",
    "\n",
    "dataset_valid = NSynth(NSynth_valid_audio_path,\n",
    "                       NSynth_valid_json_data_path,\n",
    "                       categorical_field_list=['pitch'],\n",
    "                       valid_pitch_range=[60, 84],\n",
    "                       return_full_metadata=False,\n",
    "                       label_encode_categorical_data=False)\n",
    "valid_dataloader = DataLoader(dataset_valid, batch_size=5, shuffle=True)\n",
    "\n",
    "# model = CREPE(model_capacity='tiny')\n",
    "model = build_and_load_model('full')\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, pitches_midi = next(iter(valid_dataloader))\n",
    "samples = samples.to(device)\n",
    "pitches_midi = pitches_midi.to(device)\n",
    "print(pitches_midi)\n",
    "pitches_hz = model.data_helper.midi_to_hz(pitches_midi)\n",
    "print(pitches_hz)\n",
    "pitches_cents = model.data_helper.hertz_to_cents(pitches_hz)\n",
    "print(pitches_cents)\n",
    "pitches_cents_bins = model.data_helper.cents_to_bins(pitches_cents)\n",
    "print(pitches_cents_bins.shape)\n",
    "\n",
    "plt.close(\"all\")\n",
    "plt.figure()\n",
    "frequencies_hz = model.data_helper.cents_to_hz(\n",
    "    model.data_helper._to_local_average_cents_matrix.cpu())\n",
    "plt.plot(frequencies_hz,\n",
    "         pitches_cents_bins.cpu().numpy()[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = model.data_helper.get_frames(samples)\n",
    "print(samples.shape)\n",
    "print(frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a new, random batch of samples \n",
    "samples, pitches_midi = next(iter(valid_dataloader))\n",
    "# convert the pitches of each sample to hertz to obtain the ground truth target\n",
    "target_hz = model.data_helper.midi_to_hz(pitches_midi).cpu().numpy()\n",
    "\n",
    "# disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    # batched prediction on the samples\n",
    "    model.eval()\n",
    "    logits = model.forward_audio(samples.to('cuda:0'))\n",
    "    activation = torch.sigmoid(logits)\n",
    "\n",
    "# clear previous figure\n",
    "plt.close()\n",
    "# declare new figure, 2 rows, one for frequencies and one for confidences\n",
    "batch_size, num_frames = activation.shape[:2]\n",
    "fig, axes = plt.subplots(4, batch_size, sharex='col', sharey='row', figsize=(10,6))\n",
    "fig.suptitle(f\"CREPE ('{model.model_capacity}' capacity) detected frequencies -- random NSynth samples\")\n",
    "\n",
    "# convert activations to frequencies and confidence levels\n",
    "time, frequencies, confidence = [\n",
    "    t.cpu().numpy() for t in model.data_helper.interpret_activation(activation)]\n",
    "\n",
    "# iterate over the batch of samples\n",
    "for i, (sample_time, sample_frequencies, sample_confidence) in enumerate(zip(time, frequencies, confidence)):\n",
    "    # TF implementation predictions\n",
    "    sample_time_tf, sample_frequencies_tf, sample_confidence_tf, sample_activation_tf = crepe.predict(\n",
    "        samples[i].cpu().numpy(), sr=model.fs_hz, model_capacity=model.model_capacity,\n",
    "        step_size=model.hop_length_s*1000, backend='tf'\n",
    "    )\n",
    "    # plot the detected frequencies\n",
    "    axes[0, i].plot(sample_time, sample_frequencies, color='cornflowerblue')\n",
    "    axes[1, i].plot(sample_time, sample_frequencies_tf, color='yellowgreen')\n",
    "    # add the target as a horizontal line\n",
    "    axes[0, i].plot(sample_time, [target_hz[i]] * num_frames, color='red', linestyle='dotted',)    \n",
    "    axes[1, i].plot(sample_time, [target_hz[i]] * num_frames, color='red', linestyle='dotted',)    \n",
    "    # plot the confidence levels on the second row\n",
    "    axes[2, i].plot(sample_time, sample_confidence, color='cornflowerblue')\n",
    "    axes[3, i].plot(sample_time, sample_confidence_tf, color='yellowgreen')\n",
    "\n",
    "    \n",
    "for i, ax in enumerate(axes.flat[:batch_size]):\n",
    "    ax.set_title(f'Sample {i}')\n",
    "    \n",
    "for i, ax in enumerate(axes.flat[:2*batch_size]):\n",
    "    ax.set(ylabel=f'Frequency (Hz)\\n' + ('(PyTorch)' if i < batch_size else '(TensorFlow)'))\n",
    "\n",
    "for i, ax in enumerate(axes.flat[2*batch_size:]):\n",
    "    ax.set(ylabel=f'Confidence\\n'  + ('(PyTorch)' if i < batch_size else '(TensorFlow)'))\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axes.flat:\n",
    "    ax.set(xlabel='Time (s)')\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "with torch.no_grad():\n",
    "    # obtain a new, random batch of samples \n",
    "    samples, pitches_midi = next(iter(valid_dataloader))\n",
    "\n",
    "    # batched prediction on the samples\n",
    "    model.eval()\n",
    "    logits = model.forward_audio(samples.to('cuda:0'))\n",
    "    activation = torch.sigmoid(logits)\n",
    "\n",
    "    targets = model.data_helper.make_targets(samples, pitches_midi.to('cuda:0'))\n",
    "    print(activation.shape)\n",
    "    print(targets.argmax(-1).float().std())\n",
    "    loss = criterion(logits, targets)\n",
    "    print(loss)\n",
    "\n",
    "plt.close()\n",
    "plt.figure()\n",
    "bin_frequencies_hz = model.data_helper.cents_to_hz(model.data_helper._to_local_average_cents_matrix).cpu()\n",
    "for i, (predicted, target) in enumerate(zip(activation[:, 200], targets[:, 200])):\n",
    "    plt.plot(bin_frequencies_hz, predicted.cpu(), label=f'predicted, sample {i}')\n",
    "    plt.plot(bin_frequencies_hz, target.cpu(), label=f'target, sample {i}', linestyle='dotted')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
